{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Projecte de GDSA: batchsize i weights.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRxVIQPrHF40"
      },
      "source": [
        "# Projecte de GDSA\n",
        "## Batch size i inicialització de pesos.\n",
        "Equip 3:\n",
        "Nil Torrents, Adrián Rodríguez, Eugeni Valentí, Roger Vallejo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc0o6oQ_HHmM"
      },
      "source": [
        "# Random seed 123 from numpy\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from google.colab import files\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqOR_3pBlY6U"
      },
      "source": [
        "## Definició d'hiperparàmetres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR4-Ml6VbDbu"
      },
      "source": [
        "# Let's define some hyper-parameters\n",
        "hparams = {\n",
        "    'batch_size':64,\n",
        "    'num_epochs':10,\n",
        "    'test_batch_size':64,\n",
        "    'hidden_size':128,\n",
        "    'num_classes':10,\n",
        "    'num_inputs':784,\n",
        "    'learning_rate':1e-3,\n",
        "    'log_interval':100,\n",
        "}\n",
        "\n",
        "hparams_batchSize1000 = {\n",
        "    'batch_size':1000,\n",
        "    'num_epochs':10,\n",
        "    'test_batch_size':1000,\n",
        "    'hidden_size':128,\n",
        "    'num_classes':10,\n",
        "    'num_inputs':784,\n",
        "    'learning_rate':1e-3,\n",
        "    'log_interval':100,\n",
        "}\n",
        "\n",
        "hparams_batchSize500 = {\n",
        "    'batch_size':500,\n",
        "    'num_epochs':10,\n",
        "    'test_batch_size':500,\n",
        "    'hidden_size':128,\n",
        "    'num_classes':10,\n",
        "    'num_inputs':784,\n",
        "    'learning_rate':1e-3,\n",
        "    'log_interval':100,\n",
        "}\n",
        "\n",
        "hparams_batchSize1 = {\n",
        "    'batch_size':1,\n",
        "    'num_epochs':1,\n",
        "    'test_batch_size':1,\n",
        "    'hidden_size':128,\n",
        "    'num_classes':10,\n",
        "    'num_inputs':784,\n",
        "    'learning_rate':1e-3,\n",
        "    'log_interval':100,\n",
        "}\n",
        "\n",
        "# we select to work on GPU if it is available in the machine, otherwise\n",
        "# will run on CPU\n",
        "hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "hparams_batchSize1000['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "hparams_batchSize1['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQPfwo_hl99c"
      },
      "source": [
        "## Definició de Dataset i DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wA_BAvOarC1"
      },
      "source": [
        "mnist_trainset = datasets.MNIST('data', train=True, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ]))\n",
        "mnist_testset = datasets.MNIST('data', train=False, \n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
        "                               ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    mnist_trainset,\n",
        "    batch_size=hparams['batch_size'], \n",
        "    shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    mnist_testset,\n",
        "    batch_size=hparams['test_batch_size'], \n",
        "    shuffle=False)\n",
        "\n",
        "train_loader_batchSize1000 = torch.utils.data.DataLoader(\n",
        "    mnist_trainset,\n",
        "    batch_size=hparams_batchSize1000['batch_size'], \n",
        "    shuffle=True)\n",
        "\n",
        "test_loader_batchSize1000 = torch.utils.data.DataLoader(\n",
        "    mnist_testset,\n",
        "    batch_size=hparams_batchSize1000['test_batch_size'], \n",
        "    shuffle=False)\n",
        "\n",
        "train_loader_batchSize1 = torch.utils.data.DataLoader(\n",
        "    mnist_trainset,\n",
        "    batch_size=hparams_batchSize1['batch_size'], \n",
        "    shuffle=True)\n",
        "\n",
        "test_loader_batchSize1 = torch.utils.data.DataLoader(\n",
        "    mnist_testset,\n",
        "    batch_size=hparams_batchSize1['test_batch_size'], \n",
        "    shuffle=False)\n",
        "\n",
        "train_loader_batchSize500 = torch.utils.data.DataLoader(\n",
        "    mnist_trainset,\n",
        "    batch_size=hparams_batchSize500['batch_size'], \n",
        "    shuffle=True)\n",
        "\n",
        "test_loader_batchSize500 = torch.utils.data.DataLoader(\n",
        "    mnist_testset,\n",
        "    batch_size=hparams_batchSize500['test_batch_size'], \n",
        "    shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1feh0VJqk24g"
      },
      "source": [
        "# Definició de les arquitectures neuronals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0U0RdgF8bpN"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "def describe_tensor(tensor, name=''):\r\n",
        "  # Helper function to explore the attributes of a tensor object\r\n",
        "  print('-' * 30)\r\n",
        "  print('Name: ', name)\r\n",
        "  print('-' * 30)\r\n",
        "  print('data : ', tensor.data)\r\n",
        "  print('requires_grad : ', tensor.requires_grad)\r\n",
        "  print('grad: ', tensor.grad)\r\n",
        "  print('grad_fn: ', tensor.grad_fn)\r\n",
        "  print('is_leaf: ', tensor.is_leaf)\r\n",
        "  print('=' * 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkE8HbX-IY6r",
        "outputId": "0ec79e3b-4129-4562-e433-9c17bf7024c2"
      },
      "source": [
        "# Add a (1) nn.Linear hidden layer with 128 neurons, (2) a nn.ReLU, (3) the output nn.Linear and,\n",
        "# the (4) output nn.LogSoftmax\n",
        "# NOTE: Consider the 'num_inputs', 'hidden_size', and 'num_classes' parameters \n",
        "# defined above as hyper-params\n",
        "\n",
        "#TODO\n",
        "network=nn.Sequential(\n",
        "    nn.Linear(hparams['num_inputs'], hparams['hidden_size'], True),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(hparams['hidden_size'], hparams['num_classes'], True),\n",
        "    nn.LogSoftmax()\n",
        ")\n",
        "network.to(hparams['device'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (3): LogSoftmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSTS0LWVB-Nx"
      },
      "source": [
        "Inicialitzada a zeros_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q46kjo1w6T13"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "\r\n",
        "class MyNet(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "    super().__init__() # must call the superclass init first\r\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\r\n",
        "    # self.fc1 = nn.Linear(3, 20)\r\n",
        "    \r\n",
        "    self.fc1 = nn.Linear(hparams['num_inputs'], hparams['hidden_size'])\r\n",
        "    nn.init.zeros_(self.fc1.weight)\r\n",
        "    # First hidden activation\r\n",
        "    self.act1 = nn.ReLU(inplace=True)\r\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\r\n",
        "    self.fc2 = nn.Linear(hparams['hidden_size'], hparams['num_classes'])\r\n",
        "    # No activation as we make it a linear output\r\n",
        "    self.act2 = nn.Softmax(dim=None)\r\n",
        "    \r\n",
        "  def forward(self, x):\r\n",
        "    # activation of first layer is Tanh(FC1(x))\r\n",
        "    h1 = self.act1(self.fc1(x))\r\n",
        "    # output activation\r\n",
        "    y = self.fc2(h1)\r\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l8evIgr88Rx",
        "outputId": "7d4a9153-03b1-4992-b013-7bc9eb1a7448"
      },
      "source": [
        "netZeros = MyNet() #ens diu q tenim a dins d la xarxa\r\n",
        "netZeros.to(hparams['device'])\r\n",
        "print(netZeros)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyNet(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (act1): ReLU(inplace=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (act2): Softmax(dim=None)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYCnZMWs7tst",
        "outputId": "17d937fc-1614-425e-e71d-d8207ed482a5"
      },
      "source": [
        "describe_tensor(netZeros.fc1.weight, 'FC1 weight') #Nomes layer 1. podem mirar els w. Començen sent w i b aleatoris."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Name:  FC1 weight\n",
            "------------------------------\n",
            "data :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeVxdOE-CC3O"
      },
      "source": [
        "Inicialitzada a Xavier\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv0HpHuiCFOO"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "\r\n",
        "class MyNet(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "    super().__init__() # must call the superclass init first\r\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\r\n",
        "    # self.fc1 = nn.Linear(3, 20)\r\n",
        "    \r\n",
        "    self.fc1 = nn.Linear(hparams['num_inputs'], hparams['hidden_size'])\r\n",
        "    nn.init.xavier_normal_(self.fc1.weight)\r\n",
        "    # First hidden activation\r\n",
        "    self.act1 = nn.Tanh()\r\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\r\n",
        "    self.fc2 = nn.Linear(hparams['hidden_size'], hparams['num_classes'])\r\n",
        "    # No activation as we make it a linear output\r\n",
        "    self.act2 = nn.Softmax(dim=None)\r\n",
        "    \r\n",
        "  def forward(self, x):\r\n",
        "    # activation of first layer is Tanh(FC1(x))\r\n",
        "    h1 = self.act1(self.fc1(x))\r\n",
        "    # output activation\r\n",
        "    y = self.fc2(h1)\r\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PW-oVNMCSSp",
        "outputId": "b61b6033-b6ef-4f04-ffc7-244761885af7"
      },
      "source": [
        "netXavier = MyNet() #ens diu q tenim a dins d la xarxa\r\n",
        "# Send the network to the proper device (CPU or GPU)\r\n",
        "netXavier.to(hparams['device'])\r\n",
        "print(netXavier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyNet(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (act1): Tanh()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (act2): Softmax(dim=None)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21krdLywXNTz",
        "outputId": "2589ca17-73f2-429b-d63a-6c70adddd7fc"
      },
      "source": [
        "describe_tensor(netXavier.to(hparams['device'])\r\n",
        ".fc1.weight, 'FC1 weight') #Nomes layer 1. podem mirar els w. Començen sent w i b aleatoris."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Name:  FC1 weight\n",
            "------------------------------\n",
            "data :  tensor([[-0.0366,  0.0228,  0.0238,  ..., -0.0249, -0.0505, -0.0063],\n",
            "        [-0.0691,  0.0342, -0.0060,  ..., -0.0400,  0.0604,  0.0872],\n",
            "        [ 0.0350, -0.0481,  0.0140,  ...,  0.0256,  0.0626, -0.0409],\n",
            "        ...,\n",
            "        [-0.0133,  0.0751, -0.0187,  ...,  0.0126,  0.0745,  0.0653],\n",
            "        [-0.0463,  0.0692,  0.0974,  ..., -0.1017,  0.0128, -0.0070],\n",
            "        [ 0.0970, -0.0049, -0.0868,  ..., -0.0373,  0.0303,  0.0212]],\n",
            "       device='cuda:0')\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEekIpFVTCoC"
      },
      "source": [
        "batchSize=500, inicialització Zeros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe0qi75XTBzv"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "\r\n",
        "class MyNet(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "    super().__init__() # must call the superclass init first\r\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\r\n",
        "    # self.fc1 = nn.Linear(3, 20)\r\n",
        "    \r\n",
        "    self.fc1 = nn.Linear(hparams['num_inputs'], hparams['hidden_size'])\r\n",
        "    nn.init.zeros_(self.fc1.weight)\r\n",
        "    # First hidden activation\r\n",
        "    self.act1 = nn.ReLU(inplace=True)\r\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\r\n",
        "    self.fc2 = nn.Linear(hparams['hidden_size'], hparams['num_classes'])\r\n",
        "    # No activation as we make it a linear output\r\n",
        "    self.act2 = nn.Softmax(dim=None)\r\n",
        "    \r\n",
        "  def forward(self, x):\r\n",
        "    # activation of first layer is Tanh(FC1(x))\r\n",
        "    h1 = self.act1(self.fc1(x))\r\n",
        "    # output activation\r\n",
        "    y = self.fc2(h1)\r\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_CPmuTeTB0A",
        "outputId": "c7872b39-76bc-4723-d92a-95443089df59"
      },
      "source": [
        "netZeros_500 = MyNet() #ens diu q tenim a dins d la xarxa\r\n",
        "# Send the network to the proper device (CPU or GPU)\r\n",
        "netZeros_500.to(hparams['device'])\r\n",
        "print(netZeros_500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyNet(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (act1): ReLU(inplace=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (act2): Softmax(dim=None)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk43bQaUUhsI"
      },
      "source": [
        "batchSize=500, inicialització Xavier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzSqxXQkUaD7"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "\r\n",
        "class MyNet(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "    super().__init__() # must call the superclass init first\r\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\r\n",
        "    # self.fc1 = nn.Linear(3, 20)\r\n",
        "    \r\n",
        "    self.fc1 = nn.Linear(hparams['num_inputs'], hparams['hidden_size'])\r\n",
        "    nn.init.xavier_normal_(self.fc1.weight)\r\n",
        "    # First hidden activation\r\n",
        "    self.act1 = nn.Tanh()\r\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\r\n",
        "    self.fc2 = nn.Linear(hparams['hidden_size'], hparams['num_classes'])\r\n",
        "    # No activation as we make it a linear output\r\n",
        "    self.act2 = nn.Softmax(dim=None)\r\n",
        "    \r\n",
        "  def forward(self, x):\r\n",
        "    # activation of first layer is Tanh(FC1(x))\r\n",
        "    h1 = self.act1(self.fc1(x))\r\n",
        "    # output activation\r\n",
        "    y = self.fc2(h1)\r\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNx8B31zUaD_",
        "outputId": "ea5bdc85-9292-47d5-dc79-39b8e1f1b37d"
      },
      "source": [
        "netXavier_500 = MyNet() #ens diu q tenim a dins d la xarxa\r\n",
        "# Send the network to the proper device (CPU or GPU)\r\n",
        "netXavier_500.to(hparams['device'])\r\n",
        "print(netXavier_500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyNet(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (act1): Tanh()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (act2): Softmax(dim=None)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0PJJLTQpj-l",
        "outputId": "c4536c70-c2f1-481c-e988-bc119dbc7da4"
      },
      "source": [
        "def get_nn_nparams(net):\n",
        "  \"\"\" https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/6 \"\"\"\n",
        "  pp=0\n",
        "  for p in list(net.parameters()):\n",
        "      nn=1\n",
        "      for s in list(p.size()):\n",
        "          nn = nn*s\n",
        "      pp += nn\n",
        "  return pp\n",
        "  \n",
        "print(network)\n",
        "print('Num params: ', get_nn_nparams(network))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (3): LogSoftmax(dim=None)\n",
            ")\n",
            "Num params:  101770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ttsOXUH-lJT",
        "outputId": "48f9f401-05f6-45b6-af1b-23ebbf407391"
      },
      "source": [
        "# We can access all the parameters of our network with the .parameters() function, that returns an iterable\r\n",
        "# over all tunnable params we created.\r\n",
        "params = list(netZeros.parameters())\r\n",
        "for p in params:\r\n",
        "  print(p.shape)\r\n",
        "print('You should see two matrices (weights, OUTxIN) and two vectors (biases, OUT). Each pair of weight (W) and bias (b) comes from a fully connected layer.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 784])\n",
            "torch.Size([128])\n",
            "torch.Size([10, 128])\n",
            "torch.Size([10])\n",
            "You should see two matrices (weights, OUTxIN) and two vectors (biases, OUT). Each pair of weight (W) and bias (b) comes from a fully connected layer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAC-igdvkvkz"
      },
      "source": [
        "# Optimitzador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CBYSAGqp6km"
      },
      "source": [
        "# Import optmizer \n",
        "import torch.optim as optim\n",
        "\n",
        "# Import a functional API for the loss function (use this one !!)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Optimizer: RMS Prop (use the hparams['learning_rate'] previously defined)\n",
        "# https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop\n",
        "optimizer=optim.RMSprop(network.parameters(), hparams['learning_rate'])\n",
        "optimizerZeros=optim.RMSprop(netZeros.parameters(), hparams['learning_rate'])\n",
        "optimizerXavier=optim.RMSprop(netXavier.parameters(), hparams['learning_rate'])\n",
        "optimizerZeros_500=optim.RMSprop(netZeros_500.parameters(), hparams['learning_rate'])\n",
        "optimizerXavier_500=optim.RMSprop(netXavier_500.parameters(), hparams['learning_rate'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Negative Log Likelihood (NLL) Loss criterion from the functional API\n",
        "# https://pytorch.org/docs/stable/nn.functional.html#nll-loss\n",
        "criterion=F.nll_loss\n",
        "\n",
        "# Define the Accuracy metric in the function below by:\n",
        "  # (1) obtain the maximum for each predicted element in the batch to get the class (it is the maximum index of the num_classes array per batch sample) (look at torch.argmax in the PyTorch documentation)\n",
        "  # (2) compare the predicted class index with the index in its corresponding neighbor within label_batch \n",
        "  # (3) sum up the number of affirmative comparisons and return the summation\n",
        "def correct_predictions(predicted_batch, label_batch):\n",
        "  pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "  acum = pred.eq(label_batch.view_as(pred)).sum().item()\n",
        "  return acum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Sf7CGSkTIz"
      },
      "source": [
        "# Train epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rtR48e-fPI-"
      },
      "source": [
        "batchSize=64, inicialització random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaOi6FK6sLwj"
      },
      "source": [
        "def train_epoch(train_loader, network, optimizer, criterion, hparams):\n",
        "\n",
        "  # Activate the train=True flag inside the model\n",
        "  network.train()\n",
        "  \n",
        "  device = hparams['device']\n",
        "  avg_loss = None\n",
        "  avg_weight = 0.1\n",
        "\n",
        "  # For each batch\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      # TO DO\n",
        "      data = data.view(data.shape[0], -1)\n",
        "      output = network(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      if avg_loss:\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
        "      else:\n",
        "        avg_loss = loss.item()\n",
        "      optimizer.step()\n",
        "      if batch_idx % hparams['log_interval'] == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbvCSdhHfU_A"
      },
      "source": [
        "batchSize=500, inicialització random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mynI7TSTfU_C"
      },
      "source": [
        "def train_epoch_500(train_loader, network, optimizer, criterion, hparams):\n",
        "\n",
        "  # Activate the train=True flag inside the model\n",
        "  network.train()\n",
        "  \n",
        "  device = hparams['device']\n",
        "  avg_loss = None\n",
        "  avg_weight = 0.1\n",
        "\n",
        "  # For each batch\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      # TO DO\n",
        "      data = data.view(data.shape[0], -1)\n",
        "      output = network(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      if avg_loss:\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
        "      else:\n",
        "        avg_loss = loss.item()\n",
        "      optimizer.step()\n",
        "      if batch_idx % hparams['log_interval'] == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siwPZwPCAQ_Q"
      },
      "source": [
        "Entrenem batchSize=64 amb netZeros!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFrMOZwMAPDx"
      },
      "source": [
        "def train_epoch_netZeros(train_loader, netZeros, optimizerZeros, criterion, hparams):\r\n",
        "\r\n",
        "  # Activate the train=True flag inside the model\r\n",
        "  netZeros.train()\r\n",
        "  \r\n",
        "  device = hparams['device']\r\n",
        "  avg_loss = None\r\n",
        "  avg_weight = 0.1\r\n",
        "\r\n",
        "  # For each batch\r\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "      data, target = data.to(device), target.to(device)\r\n",
        "      optimizerZeros.zero_grad()\r\n",
        "      # TO DO\r\n",
        "      data = data.view(data.shape[0], -1)\r\n",
        "      output = netZeros(data)\r\n",
        "      loss = criterion(output, target)\r\n",
        "      loss.backward()\r\n",
        "      if avg_loss:\r\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\r\n",
        "      else:\r\n",
        "        avg_loss = loss.item()\r\n",
        "      optimizerZeros.step()\r\n",
        "      if batch_idx % hparams['log_interval'] == 0:\r\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\r\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\r\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av2dqIjlCfuZ"
      },
      "source": [
        "Entrenem batchSize64 amb Xavier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oplU9bs3CjOU"
      },
      "source": [
        "def train_epoch_xavier(train_loader, netXavier, optimizerXavier, criterion, hparams):\r\n",
        "\r\n",
        "  # Activate the train=True flag inside the model\r\n",
        "  netXavier.train()\r\n",
        "  \r\n",
        "  device = hparams['device']\r\n",
        "  avg_loss = None\r\n",
        "  avg_weight = 0.1\r\n",
        "\r\n",
        "  # For each batch\r\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "      data, target = data.to(device), target.to(device)\r\n",
        "      optimizerXavier.zero_grad()\r\n",
        "      # TO DO\r\n",
        "      data = data.view(data.shape[0], -1)\r\n",
        "      output = netXavier(data)\r\n",
        "      loss = criterion(output, target)\r\n",
        "      loss.backward()\r\n",
        "      if avg_loss:\r\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\r\n",
        "      else:\r\n",
        "        avg_loss = loss.item()\r\n",
        "      optimizerXavier.step()\r\n",
        "      if batch_idx % hparams['log_interval'] == 0:\r\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\r\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\r\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M4O5ulcU3Rf"
      },
      "source": [
        "batchSize=1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh_DEHrbsX1q"
      },
      "source": [
        "def train_epoch_batchSize1000(train_loader_batchSize1000, network, optimizer, criterion, hparams_batchSize1000):\n",
        "\n",
        "  # Activate the train=True flag inside the model\n",
        "  network.train()\n",
        "  \n",
        "  device = hparams['device']\n",
        "  avg_loss = None\n",
        "  avg_weight = 0.1\n",
        "\n",
        "  # For each batch\n",
        "  for batch_idx, (data, target) in enumerate(train_loader_batchSize1000):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      # TO DO\n",
        "      data = data.view(data.shape[0], -1)\n",
        "      output = network(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      if avg_loss:\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
        "      else:\n",
        "        avg_loss = loss.item()\n",
        "      optimizer.step()\n",
        "      if batch_idx % hparams['log_interval'] == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader_batchSize1000.dataset),\n",
        "              100. * batch_idx / len(train_loader_batchSize1000), loss.item()))\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHje-9iBU6Y2"
      },
      "source": [
        "batchSize=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RptpApUbuAT1"
      },
      "source": [
        "def train_epoch_batchSize1(train_loader_batchSize1, network, optimizer, criterion, hparams_batchSize1):\n",
        "\n",
        "  # Activate the train=True flag inside the model\n",
        "  network.train()\n",
        "  \n",
        "  device = hparams['device']\n",
        "  avg_loss = None\n",
        "  avg_weight = 0.1\n",
        "\n",
        "  # For each batch\n",
        "  for batch_idx, (data, target) in enumerate(train_loader_batchSize1):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      # TO DO\n",
        "      data = data.view(data.shape[0], -1)\n",
        "      output = network(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      if avg_loss:\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
        "      else:\n",
        "        avg_loss = loss.item()\n",
        "      optimizer.step()\n",
        "      if batch_idx % hparams['log_interval'] == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader_batchSize1.dataset),\n",
        "              100. * batch_idx / len(train_loader_batchSize1), loss.item()))\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27f8zcYPU9Do"
      },
      "source": [
        "batchSize=500, Zeros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yDiOe2ZVQZ3"
      },
      "source": [
        "def train_epoch_netZeros_500(train_loader, netZeros_500, optimizerZeros_500, criterion, hparams):\r\n",
        "\r\n",
        "  # Activate the train=True flag inside the model\r\n",
        "  netZeros_500.train()\r\n",
        "  \r\n",
        "  device = hparams['device']\r\n",
        "  avg_loss = None\r\n",
        "  avg_weight = 0.1\r\n",
        "\r\n",
        "  # For each batch\r\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "      data, target = data.to(device), target.to(device)\r\n",
        "      optimizerZeros_500.zero_grad()\r\n",
        "      # TO DO\r\n",
        "      data = data.view(data.shape[0], -1)\r\n",
        "      output = netZeros_500(data)\r\n",
        "      loss = criterion(output, target)\r\n",
        "      loss.backward()\r\n",
        "      if avg_loss:\r\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\r\n",
        "      else:\r\n",
        "        avg_loss = loss.item()\r\n",
        "      optimizerZeros_500.step()\r\n",
        "      if batch_idx % hparams['log_interval'] == 0:\r\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\r\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\r\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IfBvNTDVoGg"
      },
      "source": [
        "batchSize=500, Xavier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvM3aKhSVt1B"
      },
      "source": [
        "def train_epoch_xavier_500(train_loader, netXavier_500, optimizerXavier_500, criterion, hparams):\r\n",
        "\r\n",
        "  # Activate the train=True flag inside the model\r\n",
        "  netXavier_500.train()\r\n",
        "  \r\n",
        "  device = hparams['device']\r\n",
        "  avg_loss = None\r\n",
        "  avg_weight = 0.1\r\n",
        "\r\n",
        "  # For each batch\r\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "      data, target = data.to(device), target.to(device)\r\n",
        "      optimizerXavier_500.zero_grad()\r\n",
        "      # TO DO\r\n",
        "      data = data.view(data.shape[0], -1)\r\n",
        "      output = netXavier_500(data)\r\n",
        "      loss = criterion(output, target)\r\n",
        "      loss.backward()\r\n",
        "      if avg_loss:\r\n",
        "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\r\n",
        "      else:\r\n",
        "        avg_loss = loss.item()\r\n",
        "      optimizerXavier.step()\r\n",
        "      if batch_idx % hparams['log_interval'] == 0:\r\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\r\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\r\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfRvV5MVWCyB"
      },
      "source": [
        "##Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_e3qJfUe5Ry"
      },
      "source": [
        "batchSize=64, inicialització random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsZCX8ajspqf"
      },
      "source": [
        "def test_epoch(test_loader, network, hparams):\n",
        "\n",
        "    # Dectivate the train=True flag inside the model\n",
        "    network.eval()\n",
        "    \n",
        "    device = hparams['device']\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Load data and feed it through the neural network\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.shape[0], -1)\n",
        "            output = network(data)\n",
        "\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\n",
        "\n",
        "            # compute number of correct predictions in the batch\n",
        "            acc += correct_predictions(output, target)\n",
        "\n",
        "    # Average accuracy across all correct predictions batches now\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * acc / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, acc, len(test_loader.dataset), test_acc,\n",
        "        ))\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za-uAWQnfGd1"
      },
      "source": [
        "batchSize=500, inicialització random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThYVxhnnfGeI"
      },
      "source": [
        "def test_epoch_500(test_loader, network, hparams):\n",
        "\n",
        "    # Dectivate the train=True flag inside the model\n",
        "    network.eval()\n",
        "    \n",
        "    device = hparams['device']\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Load data and feed it through the neural network\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.shape[0], -1)\n",
        "            output = network(data)\n",
        "\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\n",
        "\n",
        "            # compute number of correct predictions in the batch\n",
        "            acc += correct_predictions(output, target)\n",
        "\n",
        "    # Average accuracy across all correct predictions batches now\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * acc / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, acc, len(test_loader.dataset), test_acc,\n",
        "        ))\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm4S80v0AbCv"
      },
      "source": [
        "netZeros!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K-8swkJAZKS"
      },
      "source": [
        "def test_epoch_netZeros(test_loader, netZeros, hparams):\r\n",
        "\r\n",
        "    # Dectivate the train=True flag inside the model\r\n",
        "    netZeros.eval()\r\n",
        "    \r\n",
        "    device = hparams['device']\r\n",
        "    test_loss = 0\r\n",
        "    acc = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in test_loader:\r\n",
        "\r\n",
        "            # Load data and feed it through the neural network\r\n",
        "            data, target = data.to(device), target.to(device)\r\n",
        "            data = data.view(data.shape[0], -1)\r\n",
        "            output = netZeros(data)\r\n",
        "\r\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\r\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\r\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\r\n",
        "\r\n",
        "            # compute number of correct predictions in the batch\r\n",
        "            acc += correct_predictions(output, target)\r\n",
        "\r\n",
        "    # Average accuracy across all correct predictions batches now\r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "    test_acc = 100. * acc / len(test_loader.dataset)\r\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
        "        test_loss, acc, len(test_loader.dataset), test_acc,\r\n",
        "        ))\r\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OeRd2KcWNab"
      },
      "source": [
        "netZeros, batchSize=500!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW1GOifuWNak"
      },
      "source": [
        "def test_epoch_netZeros_500(test_loader, netZeros_500, hparams):\r\n",
        "\r\n",
        "    # Dectivate the train=True flag inside the model\r\n",
        "    netZeros_500.eval()\r\n",
        "    \r\n",
        "    device = hparams['device']\r\n",
        "    test_loss = 0\r\n",
        "    acc = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in test_loader:\r\n",
        "\r\n",
        "            # Load data and feed it through the neural network\r\n",
        "            data, target = data.to(device), target.to(device)\r\n",
        "            data = data.view(data.shape[0], -1)\r\n",
        "            output = netZeros_500(data)\r\n",
        "\r\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\r\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\r\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\r\n",
        "\r\n",
        "            # compute number of correct predictions in the batch\r\n",
        "            acc += correct_predictions(output, target)\r\n",
        "\r\n",
        "    # Average accuracy across all correct predictions batches now\r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "    test_acc = 100. * acc / len(test_loader.dataset)\r\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
        "        test_loss, acc, len(test_loader.dataset), test_acc,\r\n",
        "        ))\r\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08pJ4j4ICyW-"
      },
      "source": [
        "xavier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh0u-clGCzvF"
      },
      "source": [
        "def test_epoch_xavier(test_loader, netXavier, hparams):\r\n",
        "\r\n",
        "    # Dectivate the train=True flag inside the model\r\n",
        "    netXavier.eval()\r\n",
        "    \r\n",
        "    device = hparams['device']\r\n",
        "    test_loss = 0\r\n",
        "    acc = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in test_loader:\r\n",
        "\r\n",
        "            # Load data and feed it through the neural network\r\n",
        "            data, target = data.to(device), target.to(device)\r\n",
        "            data = data.view(data.shape[0], -1)\r\n",
        "            output = netXavier(data)\r\n",
        "\r\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\r\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\r\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\r\n",
        "\r\n",
        "            # compute number of correct predictions in the batch\r\n",
        "            acc += correct_predictions(output, target)\r\n",
        "\r\n",
        "    # Average accuracy across all correct predictions batches now\r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "    test_acc = 100. * acc / len(test_loader.dataset)\r\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
        "        test_loss, acc, len(test_loader.dataset), test_acc,\r\n",
        "        ))\r\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z06hBIIZWZTg"
      },
      "source": [
        "xavier, batchSize=500!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJlUBlYfWZTv"
      },
      "source": [
        "def test_epoch_xavier_500(test_loader, netXavier_500, hparams):\r\n",
        "\r\n",
        "    # Dectivate the train=True flag inside the model\r\n",
        "    netXavier_500.eval()\r\n",
        "    \r\n",
        "    device = hparams['device']\r\n",
        "    test_loss = 0\r\n",
        "    acc = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in test_loader:\r\n",
        "\r\n",
        "            # Load data and feed it through the neural network\r\n",
        "            data, target = data.to(device), target.to(device)\r\n",
        "            data = data.view(data.shape[0], -1)\r\n",
        "            output = netXavier_500(data)\r\n",
        "\r\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\r\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\r\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\r\n",
        "\r\n",
        "            # compute number of correct predictions in the batch\r\n",
        "            acc += correct_predictions(output, target)\r\n",
        "\r\n",
        "    # Average accuracy across all correct predictions batches now\r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "    test_acc = 100. * acc / len(test_loader.dataset)\r\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
        "        test_loss, acc, len(test_loader.dataset), test_acc,\r\n",
        "        ))\r\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVlaJQPz0ygD"
      },
      "source": [
        "def test_epoch_batchSize1000(test_loader_batchSize1000, network, hparams_batchSize1000):\n",
        "\n",
        "    # Dectivate the train=True flag inside the model\n",
        "    network.eval()\n",
        "    \n",
        "    device = hparams['device']\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Load data and feed it through the neural network\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.shape[0], -1)\n",
        "            output = network(data)\n",
        "\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\n",
        "\n",
        "            # compute number of correct predictions in the batch\n",
        "            acc += correct_predictions(output, target)\n",
        "\n",
        "    # Average accuracy across all correct predictions batches now\n",
        "    test_loss /= len(test_loader_batchSize1000.dataset)\n",
        "    test_acc = 100. * acc / len(test_loader_batchSize1000.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, acc, len(test_loader_batchSize1000.dataset), test_acc,\n",
        "        ))\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtpYdAq2uH-d"
      },
      "source": [
        "def test_epoch_batchSize1(test_loader_batchSize1, network, hparams_batchSize1):\n",
        "\n",
        "    # Dectivate the train=True flag inside the model\n",
        "    network.eval()\n",
        "    \n",
        "    device = hparams['device']\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "\n",
        "            # Load data and feed it through the neural network\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            data = data.view(data.shape[0], -1)\n",
        "            output = network(data)\n",
        "\n",
        "            test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\n",
        "            # WARNING: If you are using older Torch versions, the previous call may need to be replaced by\n",
        "            # test_loss += criterion(output, target, size_average=False).item()\n",
        "\n",
        "            # compute number of correct predictions in the batch\n",
        "            acc += correct_predictions(output, target)\n",
        "\n",
        "    # Average accuracy across all correct predictions batches now\n",
        "    test_loss /= len(test_loader_batchSize1.dataset)\n",
        "    test_acc = 100. * acc / len(test_loader_batchSize1.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, acc, len(test_loader_batchSize1.dataset), test_acc,\n",
        "        ))\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI8Gseslkb6g"
      },
      "source": [
        "# Entrenaments i tests definitius"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrSGxnYHxzK"
      },
      "source": [
        "batchSize = 64, inicialització random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvlI9HzB7vVx",
        "outputId": "ffbae127-5794-4103-b9d0-29f36d23c28e"
      },
      "source": [
        "import time\n",
        "start_time1 = time.time()\n",
        "\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\n",
        "\n",
        "  # Compute & save the average training loss for the current epoch\n",
        "  train_loss = train_epoch(train_loader, network, optimizer, criterion, hparams)\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \n",
        "  test_loss,test_accuracy=test_epoch(test_loader, network, hparams)\n",
        "  test_losses.append(test_loss)\n",
        "  test_accs.append(test_accuracy)\n",
        "\n",
        "  temps_batchSize64 = (time.time() - start_time1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.261841\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.197675\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.364292\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.177752\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.108788\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.377456\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.146492\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.095700\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.336215\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.160645\n",
            "\n",
            "Test set: Average loss: 0.1352, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.075977\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.075671\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.027029\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.136358\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.099991\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.147775\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.049865\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.107740\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.128945\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.031822\n",
            "\n",
            "Test set: Average loss: 0.0958, Accuracy: 9707/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.061763\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.019649\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.051910\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.075384\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.041722\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.098269\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.039087\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.073598\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.113737\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.012227\n",
            "\n",
            "Test set: Average loss: 0.0839, Accuracy: 9733/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.017893\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.036605\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.008892\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.013421\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.022003\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.052355\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.034703\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.018091\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004056\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.013417\n",
            "\n",
            "Test set: Average loss: 0.0925, Accuracy: 9713/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.047286\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.066892\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.070402\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.016671\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.003653\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.061764\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.052870\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.010912\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.019892\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.102343\n",
            "\n",
            "Test set: Average loss: 0.0689, Accuracy: 9797/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.009319\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.006294\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.073133\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.003468\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.018234\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.025540\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.082950\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.067136\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.007056\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.021619\n",
            "\n",
            "Test set: Average loss: 0.0744, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.018492\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.022203\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.027970\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.064063\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.004751\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.006971\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.124525\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.020029\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.006572\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.063145\n",
            "\n",
            "Test set: Average loss: 0.0789, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.064283\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.004076\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.067106\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.033902\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.017132\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.027413\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.015148\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.007921\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.020104\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.042169\n",
            "\n",
            "Test set: Average loss: 0.0744, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.040107\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.049993\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.009856\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.002148\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001908\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001878\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.007721\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.015219\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.028629\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.007669\n",
            "\n",
            "Test set: Average loss: 0.0957, Accuracy: 9754/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.002956\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.080756\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.009443\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.029110\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.007918\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.003502\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002950\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.123628\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000432\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.006901\n",
            "\n",
            "Test set: Average loss: 0.0880, Accuracy: 9767/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLFZHkvweKXD"
      },
      "source": [
        "batchSize = 500, inicialització random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlM0UI9keKXM",
        "outputId": "d0998fb9-0839-4a03-9216-bdf786566e99"
      },
      "source": [
        "import time\n",
        "start_time8 = time.time()\n",
        "\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\n",
        "train_losses_500 = []\n",
        "test_losses_500 = []\n",
        "test_accs_500 = []\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\n",
        "\n",
        "  # Compute & save the average training loss for the current epoch\n",
        "  train_loss = train_epoch(train_loader, network, optimizer, criterion, hparams)\n",
        "  train_losses_500.append(train_loss)\n",
        "\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \n",
        "  test_loss,test_accuracy=test_epoch_500(test_loader, network, hparams)\n",
        "  test_losses_500.append(test_loss)\n",
        "  test_accs_500.append(test_accuracy)\n",
        "\n",
        "  temps_batchSize500 = (time.time() - start_time8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.001920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.001901\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.005644\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.001749\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.040331\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.000383\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.001324\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.000416\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.020879\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.028290\n",
            "\n",
            "Test set: Average loss: 0.0958, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.006064\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.009661\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.000407\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.000724\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.035877\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.035136\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.000137\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.008060\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.012360\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089281\n",
            "\n",
            "Test set: Average loss: 0.0942, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001666\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001818\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.003288\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.000881\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.004919\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001741\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.037796\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001258\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.013004\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.025792\n",
            "\n",
            "Test set: Average loss: 0.0876, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001868\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.007002\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.082871\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.030286\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.008647\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.006126\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.002723\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.000471\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.004384\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.002992\n",
            "\n",
            "Test set: Average loss: 0.0992, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000311\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000501\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001480\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.000950\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.000103\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.003058\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.016620\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.020604\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.006856\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.000228\n",
            "\n",
            "Test set: Average loss: 0.0988, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.001625\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.000130\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.001890\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.000084\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001998\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.010511\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.012679\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.014010\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.001346\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.004579\n",
            "\n",
            "Test set: Average loss: 0.0930, Accuracy: 9806/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.081503\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.002999\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.003402\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.000395\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.017847\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.000188\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.001720\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.046191\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.000147\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.000279\n",
            "\n",
            "Test set: Average loss: 0.1032, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000133\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002458\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.008795\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000086\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006933\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001952\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.000550\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.002102\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.004348\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.010819\n",
            "\n",
            "Test set: Average loss: 0.1291, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000958\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004381\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.005807\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000497\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001415\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001899\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.026770\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.005308\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.000038\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001430\n",
            "\n",
            "Test set: Average loss: 0.1207, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000176\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.003054\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002851\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.069937\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000607\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000954\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.012733\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.002189\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000517\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.011706\n",
            "\n",
            "Test set: Average loss: 0.1336, Accuracy: 9742/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhqliraSGghr"
      },
      "source": [
        "netZeros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HAr3JTZAgW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893389b0-7591-4e7c-c9a3-e36a2b3f7e35"
      },
      "source": [
        "import time\r\n",
        "start_time4 = time.time()\r\n",
        "\r\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\r\n",
        "train_losses_zeros = []\r\n",
        "test_losses_zeros = []\r\n",
        "test_accs_zeros = []\r\n",
        "\r\n",
        "# For each epoch\r\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\r\n",
        "\r\n",
        "  # Compute & save the average training loss for the current epoch\r\n",
        "  train_loss = train_epoch_netZeros(train_loader, netZeros, optimizerZeros, criterion, hparams)\r\n",
        "  train_losses_zeros.append(train_loss)\r\n",
        "\r\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\r\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \r\n",
        "  test_loss,test_accuracy=test_epoch_netZeros(test_loader, netZeros, hparams)\r\n",
        "  test_losses_zeros.append(test_loss)\r\n",
        "  test_accs_zeros.append(test_accuracy)\r\n",
        "\r\n",
        "  temps_batchSize64_netZeros = (time.time() - start_time4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.034656\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -1149.273315\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -2736.925293\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -4840.976074\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -7378.656738\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -9253.627930\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -13289.453125\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -16904.234375\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -21156.621094\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -24289.083984\n",
            "\n",
            "Test set: Average loss: -26894.5644, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: -25402.150391\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: -33133.031250\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -36489.167969\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: -42219.699219\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -45760.832031\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -54262.785156\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -59488.988281\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: -69530.601562\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -75431.046875\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: -86285.882812\n",
            "\n",
            "Test set: Average loss: -87515.7327, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: -87473.679688\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: -98508.164062\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -105430.054688\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: -115500.250000\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -122763.500000\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -137928.546875\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -140603.578125\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: -148445.968750\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -156077.296875\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: -180641.890625\n",
            "\n",
            "Test set: Average loss: -180821.1126, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: -178568.281250\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: -181346.328125\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -194827.500000\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: -221584.828125\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -225878.359375\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -225076.703125\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -256583.546875\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -273083.750000\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -286474.593750\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -299510.312500\n",
            "\n",
            "Test set: Average loss: -306430.0646, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: -303476.468750\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: -309482.156250\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -333641.281250\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -346955.031250\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -361216.562500\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -376830.281250\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -407898.625000\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -414912.218750\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -431623.531250\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -444655.656250\n",
            "\n",
            "Test set: Average loss: -463966.5087, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: -448416.812500\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -468923.343750\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -505451.968750\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -535224.187500\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -526743.937500\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -564881.687500\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -583011.187500\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: -622585.750000\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -606938.687500\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: -610187.812500\n",
            "\n",
            "Test set: Average loss: -653461.5477, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: -672672.687500\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: -674318.187500\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -656441.312500\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: -699826.812500\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -734253.375000\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -744635.625000\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -786094.562500\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: -827889.812500\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -830305.375000\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: -848495.250000\n",
            "\n",
            "Test set: Average loss: -874751.8828, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: -851330.250000\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: -875518.062500\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -880252.875000\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: -970669.312500\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -964308.125000\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -976353.187500\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -957700.125000\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: -1049237.000000\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -1080597.875000\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: -1102278.375000\n",
            "\n",
            "Test set: Average loss: -1127589.3590, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: -1095608.250000\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: -1110902.875000\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -1183427.750000\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: -1171704.125000\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -1225788.000000\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -1294445.500000\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -1349828.250000\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: -1258825.625000\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -1285688.125000\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: -1341655.000000\n",
            "\n",
            "Test set: Average loss: -1411985.0374, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: -1342262.375000\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: -1408108.375000\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: -1415563.125000\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: -1394294.125000\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: -1544624.375000\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: -1573223.125000\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: -1585687.500000\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: -1589423.125000\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -1608706.875000\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: -1632794.125000\n",
            "\n",
            "Test set: Average loss: -1727822.8908, Accuracy: 1135/10000 (11%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkBL0EqMWsgu"
      },
      "source": [
        "Zeros, batchSize=500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkimKSLiWrYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf420dd5-7957-4eed-ee23-a20590d73e0a"
      },
      "source": [
        "import time\r\n",
        "start_time6 = time.time()\r\n",
        "\r\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\r\n",
        "train_losses_zeros_500 = []\r\n",
        "test_losses_zeros_500 = []\r\n",
        "test_accs_zeros_500 = []\r\n",
        "\r\n",
        "# For each epoch\r\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\r\n",
        "\r\n",
        "  # Compute & save the average training loss for the current epoch\r\n",
        "  train_loss = train_epoch_netZeros_500(train_loader, netZeros, optimizerZeros, criterion, hparams)\r\n",
        "  train_losses_zeros_500.append(train_loss)\r\n",
        "\r\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\r\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \r\n",
        "  test_loss,test_accuracy=test_epoch_netZeros_500(test_loader, netZeros, hparams)\r\n",
        "  test_losses_zeros_500.append(test_loss)\r\n",
        "  test_accs_zeros_500.append(test_accuracy)\r\n",
        "\r\n",
        "  temps_batchSize64_netZeros_500 = (time.time() - start_time6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: -1712501.500000\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -1787687.000000\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -1783018.125000\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -1848339.000000\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -1802638.000000\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -1821400.875000\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -1938707.875000\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -2059598.375000\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -1989203.250000\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -1991740.000000\n",
            "\n",
            "Test set: Average loss: -2075728.2432, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: -1926811.875000\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: -2135999.500000\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -2058953.250000\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: -2142917.750000\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -2265528.500000\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -2144633.500000\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -2254537.250000\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: -2342807.500000\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -2318969.750000\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: -2430363.250000\n",
            "\n",
            "Test set: Average loss: -2455205.5092, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: -2394815.750000\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: -2464463.000000\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -2488453.250000\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: -2661363.750000\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -2613457.250000\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -2726400.250000\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -2705014.750000\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: -2694119.500000\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -2742372.250000\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: -2871143.750000\n",
            "\n",
            "Test set: Average loss: -2865595.4600, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: -2738300.250000\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: -2998312.000000\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -2901882.250000\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: -2935864.250000\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -3088950.000000\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -3084662.000000\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -3063245.000000\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -3309998.250000\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -3421839.000000\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -3357882.500000\n",
            "\n",
            "Test set: Average loss: -3307605.0676, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: -3389963.000000\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: -3306366.500000\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -3258602.000000\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -3426670.000000\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -3465008.250000\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -3506899.500000\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -3529492.250000\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -3751644.750000\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -3623600.250000\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -3800462.500000\n",
            "\n",
            "Test set: Average loss: -3780918.7720, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: -3633333.250000\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -3690022.000000\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -3688973.750000\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -3834833.000000\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -3956780.500000\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -4169850.250000\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -3940291.000000\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: -4124065.250000\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -4069710.000000\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: -3985938.000000\n",
            "\n",
            "Test set: Average loss: -4286047.8480, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: -4332775.000000\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: -4373162.000000\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -4263809.500000\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: -4383676.500000\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -4250239.500000\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -4065468.750000\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -4560628.000000\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: -4477096.500000\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -4474296.000000\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: -4582157.000000\n",
            "\n",
            "Test set: Average loss: -4822274.2808, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: -4787080.500000\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: -4658982.000000\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -4744691.500000\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: -4995005.000000\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -4928862.000000\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -4892756.000000\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -5160759.500000\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: -5100571.000000\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -5078809.000000\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: -5301403.500000\n",
            "\n",
            "Test set: Average loss: -5390315.7368, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: -5361588.500000\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: -5444026.500000\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -5307594.500000\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: -5472394.500000\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -5555786.500000\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -5504550.000000\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -5822860.500000\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: -5932626.000000\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -5728500.500000\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: -6052115.000000\n",
            "\n",
            "Test set: Average loss: -5990008.7272, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: -6425772.500000\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: -6055095.500000\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: -5971612.500000\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: -6140777.000000\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: -6164310.500000\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: -6505080.500000\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: -6199882.500000\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: -6382124.000000\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -6643713.500000\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: -6227977.000000\n",
            "\n",
            "Test set: Average loss: -6620993.1144, Accuracy: 1135/10000 (11%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgxAAiXZDCPF"
      },
      "source": [
        "xavier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QreswdDbDDZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a700e9eb-5e53-49cd-e0d3-d0aab85a1bab"
      },
      "source": [
        "import time\r\n",
        "start_time5 = time.time()\r\n",
        "\r\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\r\n",
        "train_losses_xavier = []\r\n",
        "test_losses_xavier = []\r\n",
        "test_accs_xavier = []\r\n",
        "\r\n",
        "# For each epoch\r\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\r\n",
        "\r\n",
        "  # Compute & save the average training loss for the current epoch\r\n",
        "  train_loss = train_epoch_xavier(train_loader, netXavier, optimizerXavier, criterion, hparams)\r\n",
        "  train_losses_xavier.append(train_loss)\r\n",
        "\r\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\r\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \r\n",
        "  test_loss,test_accuracy=test_epoch_xavier(test_loader, netXavier, hparams)\r\n",
        "  test_losses_xavier.append(test_loss)\r\n",
        "  test_accs_xavier.append(test_accuracy)\r\n",
        "\r\n",
        "  temps_batchSize64_xavier = (time.time() - start_time5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.074014\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -25.195967\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -38.931900\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -50.573986\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -62.731064\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -74.540855\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -85.354576\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -98.695915\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -109.813400\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -119.050858\n",
            "\n",
            "Test set: Average loss: -125.6080, Accuracy: 8827/10000 (88%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: -125.132309\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: -137.197037\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -148.016693\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: -160.532120\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -171.947357\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -185.571426\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -197.371674\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: -208.458496\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -219.681000\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: -231.820557\n",
            "\n",
            "Test set: Average loss: -235.0301, Accuracy: 9042/10000 (90%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: -235.860229\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: -242.151581\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -254.029938\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: -272.742401\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -281.723053\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -293.437164\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -309.685303\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: -313.715607\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -326.093872\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: -332.971649\n",
            "\n",
            "Test set: Average loss: -344.2970, Accuracy: 9054/10000 (91%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: -343.997925\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: -353.748596\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -370.123810\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: -370.706055\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -391.874054\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -404.851135\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -411.722412\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -424.595123\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -433.326904\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -448.582886\n",
            "\n",
            "Test set: Average loss: -453.7224, Accuracy: 9089/10000 (91%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: -455.246185\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: -455.885468\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -471.193420\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -496.359100\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -503.964691\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -518.391846\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -528.596741\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -530.394104\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -546.338928\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -549.384583\n",
            "\n",
            "Test set: Average loss: -563.1192, Accuracy: 9089/10000 (91%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: -567.363098\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -572.630493\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -581.248108\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -599.376282\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -611.547424\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -623.628052\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -638.255188\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: -649.604431\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -650.462341\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: -670.167236\n",
            "\n",
            "Test set: Average loss: -672.4507, Accuracy: 9090/10000 (91%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: -671.061768\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: -682.304077\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -699.507507\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: -703.542847\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -714.095093\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -731.044312\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -740.009705\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: -755.628052\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -764.061462\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: -780.099426\n",
            "\n",
            "Test set: Average loss: -782.0237, Accuracy: 9097/10000 (91%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: -771.853638\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: -793.112488\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -804.593445\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: -824.152527\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -828.731750\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -836.234985\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -859.983398\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: -865.405762\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -880.713135\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: -884.912598\n",
            "\n",
            "Test set: Average loss: -891.6814, Accuracy: 9098/10000 (91%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: -895.912170\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: -900.417297\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -912.209473\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: -929.001648\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -938.649353\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -953.718628\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -972.055420\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: -971.173523\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -978.949097\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: -992.914429\n",
            "\n",
            "Test set: Average loss: -1001.4600, Accuracy: 9096/10000 (91%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: -1009.768494\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: -1026.593262\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: -1016.499573\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: -1036.684570\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: -1060.388062\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: -1060.606567\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: -1074.962280\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: -1095.422241\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -1100.985718\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: -1103.498047\n",
            "\n",
            "Test set: Average loss: -1110.8852, Accuracy: 9100/10000 (91%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJaRrDEVXJ2l"
      },
      "source": [
        "Xavier, batchSize=500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJeHVk7yXJ2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860063f8-c1d8-4733-82dd-55a13637c66f"
      },
      "source": [
        "import time\r\n",
        "start_time7 = time.time()\r\n",
        "\r\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\r\n",
        "train_losses_xavier_500 = []\r\n",
        "test_losses_xavier_500 = []\r\n",
        "test_accs_xavier_500 = []\r\n",
        "\r\n",
        "# For each epoch\r\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\r\n",
        "\r\n",
        "  # Compute & save the average training loss for the current epoch\r\n",
        "  train_loss = train_epoch_xavier_500(train_loader, netXavier, optimizerXavier, criterion, hparams)\r\n",
        "  train_losses_xavier_500.append(train_loss)\r\n",
        "\r\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\r\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \r\n",
        "  test_loss,test_accuracy=test_epoch_xavier(test_loader, netXavier, hparams)\r\n",
        "  test_losses_xavier_500.append(test_loss)\r\n",
        "  test_accs_xavier_500.append(test_accuracy)\r\n",
        "\r\n",
        "  temps_batchSize64_xavier_500 = (time.time() - start_time7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: -1100.501831\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -1134.852295\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -1130.218872\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -1150.363770\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -1176.216431\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -1168.515137\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -1175.393311\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -1196.521118\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -1220.320435\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -1222.142944\n",
            "\n",
            "Test set: Average loss: -1220.3821, Accuracy: 9096/10000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: -1227.137329\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: -1247.003662\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -1247.610352\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: -1252.705811\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -1252.266113\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -1296.231689\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -1284.385132\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: -1283.662109\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -1320.380615\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: -1325.418823\n",
            "\n",
            "Test set: Average loss: -1329.8052, Accuracy: 9088/10000 (91%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: -1274.408936\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: -1358.264282\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -1352.237671\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: -1377.026978\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -1390.762329\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -1405.214722\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -1423.752563\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: -1421.042725\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -1427.764648\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: -1432.739380\n",
            "\n",
            "Test set: Average loss: -1439.2525, Accuracy: 9095/10000 (91%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: -1423.054565\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: -1422.579834\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -1449.718628\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: -1488.821899\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -1484.538574\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -1518.663940\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -1500.392334\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -1529.685303\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -1561.223389\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -1546.713257\n",
            "\n",
            "Test set: Average loss: -1548.4636, Accuracy: 9120/10000 (91%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: -1545.703247\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: -1556.848389\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -1568.092773\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -1578.739136\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -1621.153076\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -1578.023193\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -1611.715698\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -1637.404297\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -1643.669556\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -1658.340698\n",
            "\n",
            "Test set: Average loss: -1658.0994, Accuracy: 9108/10000 (91%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: -1666.699829\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -1689.021362\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -1688.179199\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -1712.873901\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -1708.103027\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -1733.191040\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -1708.757935\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: -1732.473877\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -1758.598389\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: -1766.497925\n",
            "\n",
            "Test set: Average loss: -1768.0610, Accuracy: 9104/10000 (91%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: -1760.348877\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: -1777.735962\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -1768.461914\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: -1816.174316\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -1812.935913\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -1831.960571\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -1836.440430\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: -1863.923096\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -1876.115601\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: -1879.450928\n",
            "\n",
            "Test set: Average loss: -1877.9155, Accuracy: 9109/10000 (91%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: -1871.711182\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: -1914.244141\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -1906.382568\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: -1927.229492\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -1930.709229\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -1950.311401\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -1953.328247\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: -1967.888916\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -1971.804810\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: -1966.440552\n",
            "\n",
            "Test set: Average loss: -1986.5175, Accuracy: 9101/10000 (91%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: -2000.403076\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: -2001.577881\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -1993.932861\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: -2024.841675\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -2037.359009\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -2045.390259\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -2082.669678\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: -2073.634033\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -2076.178223\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: -2098.999268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNI7B4GXbqPz"
      },
      "source": [
        "batchSize=1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cvABslc1INP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a233f9-7537-40fc-bbdf-6fa0ebaeffa0"
      },
      "source": [
        "start_time2 = time.time()\n",
        "\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\n",
        "train_losses_batchSize1000 = []\n",
        "test_losses_batchSize1000 = []\n",
        "test_accs_batchSize1000 = []\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(1, hparams['num_epochs'] + 1):\n",
        "\n",
        "# Compute & save the average training loss for the current epoch\n",
        "  train_loss_batchSize1000 = train_epoch_batchSize1000(train_loader_batchSize1000, network, optimizer, criterion, hparams_batchSize1000)\n",
        "  train_losses_batchSize1000.append(train_loss_batchSize1000)\n",
        "\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \n",
        "  test_loss_batchSize1000,test_accuracy_batchSize1000=test_epoch_batchSize1000(test_loader_batchSize1000, network, hparams_batchSize1000)\n",
        "  test_losses_batchSize1000.append(test_loss_batchSize1000)\n",
        "  test_accs_batchSize1000.append(test_accuracy_batchSize1000)\n",
        "\n",
        "  temps_batchSize1000 = (time.time() - start_time2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.013345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0997, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.001212\n",
            "\n",
            "Test set: Average loss: 0.0982, Accuracy: 9823/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.000804\n",
            "\n",
            "Test set: Average loss: 0.0982, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.000498\n",
            "\n",
            "Test set: Average loss: 0.0981, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000476\n",
            "\n",
            "Test set: Average loss: 0.0975, Accuracy: 9819/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.000282\n",
            "\n",
            "Test set: Average loss: 0.0979, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000220\n",
            "\n",
            "Test set: Average loss: 0.0979, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000241\n",
            "\n",
            "Test set: Average loss: 0.0979, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000118\n",
            "\n",
            "Test set: Average loss: 0.0986, Accuracy: 9817/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000161\n",
            "\n",
            "Test set: Average loss: 0.0989, Accuracy: 9819/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wNqsCEcuPpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def82e41-3db4-4ff6-ee59-bf7234913e50"
      },
      "source": [
        "start_time3 = time.time()\n",
        "\n",
        "# Init lists to save the evolution of the training & test losses/accuracy.\n",
        "train_losses_batchSize1 = []\n",
        "test_losses_batchSize1 = []\n",
        "test_accs_batchSize1 = []\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(1, hparams_batchSize1['num_epochs'] + 1):\n",
        "\n",
        "# Compute & save the average training loss for the current epoch\n",
        "  train_loss_batchSize1 = train_epoch_batchSize1(train_loader_batchSize1, network, optimizer, criterion, hparams_batchSize1)\n",
        "  train_losses_batchSize1.append(train_loss_batchSize1)\n",
        "\n",
        "  # TODO: Compute & save the average test loss & accuracy for the current epoch\n",
        "  # TIP: Review the functions previously defined to implement the train/test epochs \n",
        "  test_loss_batchSize1,test_accuracy_batchSize1=test_epoch_batchSize1(test_loader_batchSize1, network, hparams_batchSize1)\n",
        "  test_losses_batchSize1.append(test_loss_batchSize1)\n",
        "  test_accs_batchSize1.append(test_accuracy_batchSize1)\n",
        "\n",
        "  temps_batchSize1 = (time.time() - start_time3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [100/60000 (0%)]\tLoss: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [200/60000 (0%)]\tLoss: 0.022612\n",
            "Train Epoch: 1 [300/60000 (0%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [400/60000 (1%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [500/60000 (1%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [600/60000 (1%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [700/60000 (1%)]\tLoss: 0.000021\n",
            "Train Epoch: 1 [800/60000 (1%)]\tLoss: 0.002538\n",
            "Train Epoch: 1 [900/60000 (2%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1100/60000 (2%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1300/60000 (2%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1400/60000 (2%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1500/60000 (2%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1700/60000 (3%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 4.613550\n",
            "Train Epoch: 1 [1900/60000 (3%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.000006\n",
            "Train Epoch: 1 [2100/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2200/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2300/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2500/60000 (4%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [2600/60000 (4%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2700/60000 (4%)]\tLoss: 0.002531\n",
            "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [2900/60000 (5%)]\tLoss: 2.661897\n",
            "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3100/60000 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3300/60000 (6%)]\tLoss: 2.728706\n",
            "Train Epoch: 1 [3400/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3500/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3700/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3800/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [3900/60000 (6%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.000003\n",
            "Train Epoch: 1 [4100/60000 (7%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4300/60000 (7%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4400/60000 (7%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4500/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4600/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4700/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [4900/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5100/60000 (8%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5200/60000 (9%)]\tLoss: 9.180705\n",
            "Train Epoch: 1 [5300/60000 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [5500/60000 (9%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 0.001475\n",
            "Train Epoch: 1 [5700/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5800/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [5900/60000 (10%)]\tLoss: 0.000069\n",
            "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6100/60000 (10%)]\tLoss: 0.000045\n",
            "Train Epoch: 1 [6200/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6300/60000 (10%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6500/60000 (11%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6600/60000 (11%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6700/60000 (11%)]\tLoss: 0.002890\n",
            "Train Epoch: 1 [6800/60000 (11%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [6900/60000 (12%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7100/60000 (12%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7300/60000 (12%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7400/60000 (12%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7500/60000 (12%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7600/60000 (13%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7700/60000 (13%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7800/60000 (13%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [7900/60000 (13%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8100/60000 (14%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8200/60000 (14%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8300/60000 (14%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8500/60000 (14%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8600/60000 (14%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8700/60000 (14%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [8900/60000 (15%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.000005\n",
            "Train Epoch: 1 [9100/60000 (15%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [9200/60000 (15%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [9300/60000 (16%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [9400/60000 (16%)]\tLoss: 0.000826\n",
            "Train Epoch: 1 [9500/60000 (16%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [9700/60000 (16%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [9800/60000 (16%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [9900/60000 (16%)]\tLoss: 0.000305\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [10100/60000 (17%)]\tLoss: 0.000008\n",
            "Train Epoch: 1 [10200/60000 (17%)]\tLoss: 0.010443\n",
            "Train Epoch: 1 [10300/60000 (17%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [10400/60000 (17%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [10500/60000 (18%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [10600/60000 (18%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [10700/60000 (18%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [10800/60000 (18%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [10900/60000 (18%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [11100/60000 (18%)]\tLoss: 0.000348\n",
            "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [11300/60000 (19%)]\tLoss: 0.000019\n",
            "Train Epoch: 1 [11400/60000 (19%)]\tLoss: 0.005532\n",
            "Train Epoch: 1 [11500/60000 (19%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [11600/60000 (19%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [11700/60000 (20%)]\tLoss: 0.000079\n",
            "Train Epoch: 1 [11800/60000 (20%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [11900/60000 (20%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12100/60000 (20%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12200/60000 (20%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12300/60000 (20%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12400/60000 (21%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12500/60000 (21%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12600/60000 (21%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12700/60000 (21%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [12900/60000 (22%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13100/60000 (22%)]\tLoss: 0.003341\n",
            "Train Epoch: 1 [13200/60000 (22%)]\tLoss: 0.000001\n",
            "Train Epoch: 1 [13300/60000 (22%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13400/60000 (22%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13500/60000 (22%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13600/60000 (23%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13700/60000 (23%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13800/60000 (23%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [13900/60000 (23%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [14100/60000 (24%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [14200/60000 (24%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [14300/60000 (24%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.000005\n",
            "Train Epoch: 1 [14500/60000 (24%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [14600/60000 (24%)]\tLoss: 0.075977\n",
            "Train Epoch: 1 [14700/60000 (24%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [14800/60000 (25%)]\tLoss: 2.849404\n",
            "Train Epoch: 1 [14900/60000 (25%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15100/60000 (25%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15200/60000 (25%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15300/60000 (26%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15400/60000 (26%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15500/60000 (26%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15600/60000 (26%)]\tLoss: 2.725833\n",
            "Train Epoch: 1 [15700/60000 (26%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15800/60000 (26%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [15900/60000 (26%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.000151\n",
            "Train Epoch: 1 [16100/60000 (27%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [16200/60000 (27%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [16300/60000 (27%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [16400/60000 (27%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [16500/60000 (28%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [16600/60000 (28%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [16700/60000 (28%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [16800/60000 (28%)]\tLoss: 0.458704\n",
            "Train Epoch: 1 [16900/60000 (28%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17100/60000 (28%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17200/60000 (29%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17300/60000 (29%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17400/60000 (29%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17500/60000 (29%)]\tLoss: 0.000025\n",
            "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17700/60000 (30%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17800/60000 (30%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [17900/60000 (30%)]\tLoss: 2.666285\n",
            "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18100/60000 (30%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18200/60000 (30%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18300/60000 (30%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18400/60000 (31%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18500/60000 (31%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18600/60000 (31%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18700/60000 (31%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18800/60000 (31%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [18900/60000 (32%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19100/60000 (32%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19300/60000 (32%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19400/60000 (32%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19500/60000 (32%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19600/60000 (33%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19700/60000 (33%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [19800/60000 (33%)]\tLoss: 0.000002\n",
            "Train Epoch: 1 [19900/60000 (33%)]\tLoss: 0.000030\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [20100/60000 (34%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [20200/60000 (34%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [20300/60000 (34%)]\tLoss: 0.000874\n",
            "Train Epoch: 1 [20400/60000 (34%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [20500/60000 (34%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [20600/60000 (34%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [20700/60000 (34%)]\tLoss: 0.000036\n",
            "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.000000\n",
            "Train Epoch: 1 [20900/60000 (35%)]\tLoss: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKg78Om7vL9Q"
      },
      "source": [
        "import datetime\n",
        "print('Per a batch_size = 64, inicialització random, tenim ' + str(datetime.timedelta(seconds=temps_batchSize64)))\n",
        "print('Per a batch_size = 64, inicialització a Zeros, tenim ' + str(datetime.timedelta(seconds=temps_batchSize64_netZeros)))\n",
        "print('Per a batch_size = 64, inicialització a Xavier, tenim ' + str(datetime.timedelta(seconds=temps_batchSize64_xavier)))\n",
        "print()\n",
        "print('Per a batch_size = 1000, inicialització random, tenim ' + str(datetime.timedelta(seconds=temps_batchSize1000)))\n",
        "print()\n",
        "print('Per a batch_size = 500, inicialització random, tenim ' + str(datetime.timedelta(seconds=temps_batchSize500)))\n",
        "print('Per a batch_size = 500, inicialització a Xavier, tenim ' + str(datetime.timedelta(seconds=temps_batchSize64_xavier_500)))\n",
        "print('Per a batch_size = 500, inicialització a Zeros, tenim ' + str(datetime.timedelta(seconds=temps_batchSize64_netZeros_500)))\n",
        "print()\n",
        "print('Per a batch_size = 1, i només una època, tenim ' + str(datetime.timedelta(seconds=temps_batchSize1)))\n",
        "print('    Multiplicant per a 10 èpoques, equival a ' + str(datetime.timedelta(seconds=10*temps_batchSize1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YolyyJQVZrS1"
      },
      "source": [
        "plt.figure()\r\n",
        "\r\n",
        "data =('Batch Size = 64, random', 'Batch Size = 64, zeros', 'Batch Size = 64, xavier')\r\n",
        "pos = np.arange(len(data))\r\n",
        "temps = [temps_batchSize64, temps_batchSize64_netZeros, temps_batchSize64_xavier]\r\n",
        "\r\n",
        "# change the bar color to be less bright blue\r\n",
        "bars = plt.bar(pos, temps, align='center', linewidth=0, color='lightslategrey')\r\n",
        "# make one bar, the python bar, a contrasting color\r\n",
        "bars[2].set_color('#1F77B4')\r\n",
        "\r\n",
        "# soften all labels by turning grey\r\n",
        "plt.xticks(pos, data, alpha=0.8, rotation = 45)\r\n",
        "# remove the Y label since bars are directly labeled\r\n",
        "#plt.ylabel('% Popularity', alpha=0.8)\r\n",
        "plt.title('Execution times when batch size = 64 for MNIST train + test', alpha=.9)\r\n",
        "\r\n",
        "# remove all the ticks (both axes), and tick labels on the Y axis\r\n",
        "plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')\r\n",
        "\r\n",
        "# remove the frame of the chart\r\n",
        "for spine in plt.gca().spines.values():\r\n",
        "    spine.set_visible(False)\r\n",
        "    \r\n",
        "# direct label each bar with Y axis values\r\n",
        "for bar in bars:\r\n",
        "    plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height() - 5, str(int(bar.get_height())) + ' sec', \r\n",
        "                 ha='center', color='w', fontsize=11)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xdj1JGheS8q"
      },
      "source": [
        "plt.figure()\r\n",
        "\r\n",
        "data = ('Batch Size = 500, random', 'Batch Size = 500, zeros', 'Batch Size = 500, xavier')\r\n",
        "pos = np.arange(len(data))\r\n",
        "temps = [temps_batchSize500, temps_batchSize64_netZeros_500, temps_batchSize64_xavier_500]\r\n",
        "\r\n",
        "# change the bar color to be less bright blue\r\n",
        "bars = plt.bar(pos, temps, align='center', linewidth=0, color='lightslategrey')\r\n",
        "# make one bar, the python bar, a contrasting color\r\n",
        "bars[2].set_color('#1F77B4')\r\n",
        "\r\n",
        "# soften all labels by turning grey\r\n",
        "plt.xticks(pos, data, alpha=0.8, rotation = 45)\r\n",
        "# remove the Y label since bars are directly labeled\r\n",
        "#plt.ylabel('% Popularity', alpha=0.8)\r\n",
        "plt.title('Execution times when batch size = 500 for MNIST train + test', alpha=0.8)\r\n",
        "\r\n",
        "# remove all the ticks (both axes), and tick labels on the Y axis\r\n",
        "plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')\r\n",
        "\r\n",
        "# remove the frame of the chart\r\n",
        "for spine in plt.gca().spines.values():\r\n",
        "    spine.set_visible(False)\r\n",
        "    \r\n",
        "# direct label each bar with Y axis values\r\n",
        "for bar in bars:\r\n",
        "    plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height() - 5, str(int(bar.get_height())) + ' sec', \r\n",
        "                 ha='center', color='w', fontsize=11)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMyCUkl8Jfb9"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('batch_size = 64', fontsize=15)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(0,0.2)\n",
        "plt.ylabel('NLLLoss')\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(test_losses, label='test')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.title('batch_size = 64', fontsize=15)\n",
        "plt.ylim(95,100)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy [%]')\n",
        "plt.plot(test_accs)\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('batch_size = 1000', fontsize=15)\n",
        "plt.ylim(0,0.2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('NLLLoss')\n",
        "plt.plot(train_losses_batchSize1000, label='train')\n",
        "plt.plot(test_losses_batchSize1000, label='test')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.title('batch_size = 1000', fontsize=15)\n",
        "plt.ylim(95,100)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy [%]')\n",
        "plt.plot(test_accs_batchSize1000)\n",
        "\n",
        "espai = 0.3\n",
        "plt.subplots_adjust(hspace=espai, wspace=espai)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-x5JedhGuYX"
      },
      "source": [
        "## Plots d'inicialitzacions de pesos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlIzgkdXGtuI"
      },
      "source": [
        "plt.figure(figsize=(26,6))\r\n",
        "\r\n",
        "plt.subplot(1,3,1)\r\n",
        "plt.title('batch_size = 64, inicialització random', fontsize=15)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylim(85,100)\r\n",
        "plt.ylabel('Test Accuracy [%]')\r\n",
        "plt.plot(test_accs)\r\n",
        "\r\n",
        "plt.subplot(1,3,2)\r\n",
        "plt.title('batch_size = 64, inicialització a Xavier', fontsize=15)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylim(85,100)\r\n",
        "plt.ylabel('Test Accuracy [%]')\r\n",
        "plt.plot(test_accs_xavier)\r\n",
        "\r\n",
        "plt.subplot(1,3,3)\r\n",
        "plt.text(3, 14, s=\"L'eix de les y és diferent! 10-15%.\", bbox=dict(facecolor='red', alpha=0.5), fontsize=15)\r\n",
        "plt.title('batch_size = 64, inicialització a Zeros', fontsize=15)\r\n",
        "plt.ylim(10,15)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Test Accuracy [%]')\r\n",
        "plt.plot(test_accs_zeros)\r\n",
        "\r\n",
        "espai = 0.3\r\n",
        "plt.subplots_adjust(hspace=espai, wspace=espai)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFUDOBpoKOvj"
      },
      "source": [
        "plt.figure(figsize=(30,6))\r\n",
        "plt.subplot(1,3,1)\r\n",
        "plt.title('batch_size = 500, inicialització random', fontsize=15)\r\n",
        "plt.ylim(85,100)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Test Accuracy [%]')\r\n",
        "plt.plot(test_accs_500)\r\n",
        "\r\n",
        "plt.subplot(1,3,2)\r\n",
        "plt.title('batch_size = 500, inicialització a Xavier', fontsize=15)\r\n",
        "plt.ylim(85,100)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Test Accuracy [%]')\r\n",
        "plt.plot(test_accs_xavier_500)\r\n",
        "\r\n",
        "\r\n",
        "plt.subplot(1,3,3)\r\n",
        "plt.text(3, 14, s=\"L'eix de les y és diferent! 10-15%.\", bbox=dict(facecolor='red', alpha=0.5), fontsize=15)\r\n",
        "plt.title('batch_size = 500, inicialització a Zeros', fontsize=15)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylim(10,15)\r\n",
        "plt.ylabel('Test Accuracy [%]')\r\n",
        "plt.plot(test_accs_zeros_500)\r\n",
        "\r\n",
        "espai = 0.3\r\n",
        "plt.subplots_adjust(hspace=espai, wspace=espai)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}